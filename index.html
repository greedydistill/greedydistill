<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
  <meta name="description"
        content="Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency">
  <title>Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency</title>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://loopyavatar.github.io/">
                        Loopy
                    </a>
                    <a class="navbar-item" href="https://cyberhost.github.io/">
                        CyberHost
                    </a>
                </div>
            </div>
        </div>

    </div>
  </nav>

  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <h1> Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency </h1>
      <span>
        <a target="_blank" href="" onclick="return false;">Jianwen&nbsp;Jiang</a><sup>1*†</sup>,
        <a target="_blank" href="" onclick="return false;">Chao&nbsp;Liang</a><sup>1*</sup>,
        <a target="_blank" href="" onclick="return false;">Jiaqi&nbsp;Yang</a><sup>1*</sup>,
        <a target="_blank" href="" onclick="return false;">Gaojie&nbsp;Lin</a><sup>1</sup>,
        <a target="_blank" href="" onclick="return false;">Tianyun&nbsp;Zhong</a><sup>2‡</sup>,
        <a target="_blank" href="" onclick="return false;">Yanbo&nbsp;Zheng</a><sup>1</sup>
        <br />
      </span>
      <span><sup>1</sup>Bytedance,<sup>2</sup>Zhejiang&nbsp;University</span>
      <span><sup>*</sup>Equal contribution,<sup>†</sup>Project lead,<sup>‡</sup>Internship at Bytedance</span>
      <div class="flex flex-gap" style="margin-bottom:0.5em;">
        <a target="_blank" href="https://arxiv.org/pdf/2409.02634" ><button>Paper</button></a>
	      <!-- <a target="_blank" href="" onclick="alert('To be determined');return false;"><button>Code</button></a> -->
        <a target="_blank" href="https://loopyavatar.github.io"><button>Page</button></a>
      </div>
      <small><span><b>TL;DR</b>: we propose an end-to-end audio-only conditioned video diffusion model named <b>Loopy</b>. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference, delivering more lifelike and high-quality results across various scenarios.</span></small>
      <div class='responsive-image-container'>
        <img src='image/teaser.png' alt='' />
      </div>
    </div>

    <div id="sections" class="column-flex">
      <!-- <p style="color:#700000"><i>(Note: all portrait images on this page are virtual, non-existing identities generated by StyleGAN2 or DALL·E-3 (except for Mona Lisa). We are exploring visual affective skill generation for virtual, interactive characters, NOT impersonating any person in the real world. This is only a research demonstration and there's no product or API release plan. See also the bottom of this page for more of our Responsible AI considerations.) </i></p> -->
      <h3>Generated Videos</h3>
        <p>
          Loopy supports various visual and audio styles. It can generate vivid motion details from audio alone, such as non-speech movements like sighing, emotion-driven eyebrow and eye movements, and natural head movements. <br/>
          <b>*</b> Note that all results in this page use the first frame as <b>reference image</b> and conditioned on <b>audio only</b> without need of spatial conditions as templates.
        </p>
        <div class="video-slider">
          <video src="video/main_1.mp4"></video>
          <video src="video/main_2.mp4"></video>
          <video src="video/main_3.mp4"></video>
          <video src="video/main_4.mp4"></video>
          <video src="video/main_5.mp4"></video>
        </div>

      <h3>Motion Diversity</h3>
        <p>
          Loopy can generate motion-adapted synthesis results for the same reference image based on different audio inputs, whether they are rapid, soothing, or realistic singing performances.
        </p>
        <div class="video-slider">
          <video src="video/diverse_1.mp4"></video>
          <video src="video/diverse_5.mp4"></video>
          <video src="video/diverse_2.mp4"></video>
          <video src="video/diverse_6.mp4"></video>
          <video src="video/diverse_3.mp4"></video>
          <video src="video/diverse_7.mp4"></video>
          <video src="video/diverse_4.mp4"></video>
          <video src="video/diverse_8.mp4"></video>
        </div>
        
      <h3>Singing</h3>
        <p>Additional results demonstrating singing</p>
        <div class="video-slider">
          <video src="video/sing_1.mp4"></video>
          <video src="video/sing_2.mp4"></video>
          <video src="video/sing_3.mp4"></video>
          <video src="video/sing_4.mp4"></video>
          <video src="video/sing_5.mp4"></video>
          <video src="video/sing_6.mp4"></video>
        </div>

      <h3>More Video Results</h3>
        <p>Additional results about non-human realistic images</p>
        <div class="video-slider">
          <video src="video/nonhuman_1.mp4"></video>
          <video src="video/nonhuman_2.mp4"></video>
          <video src="video/nonhuman_3.mp4"></video>
          <video src="video/nonhuman_4.mp4"></video>
          <video src="video/nonhuman_5.mp4"></video>
        </div>

        <p>Loopy also supports input images with side profiles effectively</p>
        <div class="video-slider">
          <video src="video/side_1.mp4"></video>
          <video src="video/side_2.mp4"></video>
          <video src="video/side_3.mp4"></video>
          <video src="video/side_4.mp4"></video>
          <video src="video/side_5.mp4"></video>
        </div>

        <p>More results about realistic portrait inputs</p>
        <div class="video-slider">
          <video src="video/realistic_1.mp4"></video>
          <video src="video/realistic_2.mp4"></video>
          <video src="video/realistic_3.mp4"></video>
          <video src="video/realistic_4.mp4"></video>
          <video src="video/realistic_5.mp4"></video>
        </div>

      <h3>Comparison with Recent Methods</h3>
        <div class="video-container">
          <video controls playsInline src="video/comparison_1.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_2.mp4"></video>
        </div>

      <h3>Ethics Concerns</h3>
        <p>
          The purpose of this work is only for research. The images and audios used in these demos are from public sources. If there are any concerns, please contact us (jianwen.alan@gmail.com) and we will delete it in time.
        </p>

      <h3>Acknowledgement</h3>
        <p>
          Some figures about film and interview in <b>More Video Results</b> are from <a href="https://celebv-hq.github.io/">Celebv-HQ</a>. Some test audios and images are from <a href="https://badtobest.github.io/echomimic.html">Echomimic</a>, <a href="https://humanaigc.github.io/emote-portrait-alive/">EMO</a>, and <a href="https://www.microsoft.com/en-us/research/project/vasa-1">VASA-1</a> et al. Thanks to these great work!
        </p>
      
      <h3>BibTeX</h3>
        <pre><code>
          @article{jiang2024loopy,
            title={Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency},
            author={Jiang, Jianwen and Liang, Chao and Yang, Jiaqi and Lin, Gaojie and Zhong, Tianyun and Zheng, Yanbo},
            journal={arXiv preprint arXiv:2409.02634},
            year={2024}
          }
        </code></pre>

      <br/>
      <br/>
      <br/>
    </div>
  </div>
  <script src="index.js"></script>
  <script>
    function comming_soon_click() {
      alert('Comming soon!');
    }
    function TBD_click() {
      alert('TBD');
    }
  </script>
</body>



</html>
